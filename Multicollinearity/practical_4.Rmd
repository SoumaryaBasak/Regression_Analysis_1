---
title: "practical_4"
author: "Soumarya Basak"
date: "16/05/2022"
output:
  pdf_document:
    keep_tex: yes
  html_document:
    df_print: paged
---

# Problem 1

## a

### Importing the data

```{r}
library(readr)
df<- read.csv("C:\\Users\\souma\\Dropbox\\Mstat_CU\\Sem 2\\Regression_analysis_1\\Data Sets\\exp_inc_welath_multi.csv")
colnames(df)<- c("y","x1","x2")
colnames(df)
```
### Fit the model
```{r}
model1<- lm(y~ x1+x2,data=df)
summary(model1)
```

All the coefficients are insignificant, but the adj.r-square is very high,so there may be some multicollinearity.


### Correlation Matrix

```{r}
cor(df[,-1])
```
The correlation between `x1` and `x2` is very high, so there is a high multicollinearity.

### VIF 
```{r}
library(car)
vif(model1)

```
**Thumb Rule :** The Vif's are quite large (>> 10), so tehere is a multicollinearity in between the variables.


### CN

```{r}
# For the  Design Matrix
d<-model.matrix(model1)
d_t<- t(d) %*% d # X'X


# Finding the eigen values of matrix
lambda<-eigen(d_t)$values
lambda

# CN
cn<- sqrt(max(lambda)/min(lambda))
cn
```
The CN is too large, so there there is multicolliearity in the data.



## Remadial Measures

## b: adding two obsns......

### adding 11th and 12th observation

```{r}

a11<- c(160,120,3000)  # 11th obns
a12<- c(85,255,920)    # 12th obsn
df1<- rbind(df,a11,a12)
dim(df1)
```
### Fit the model in new data
```{r}
model2<- lm(y~. , data=df1)
summary(model2)
```
### Check for multicollinearity

```{r}
vif(model2)
```
```{r}
# design matrix
d1<- model.matrix(model2)
dd<- t(d1) %*% d1

# eigen value of matrix
lambda1<-eigen(dd)$values

#CN
cn2<- sqrt(max(lambda1)/min(lambda1))
cn2


# correlation matrix
cor(df1[,-1])
```

**Comment:** The VIF becomes low, the correlation between the variables becomes low, but the CN is very large.

**Comment:** As the Vifs are low and correlation betweeen the variables is not such high their might not be the problem of multicollinearity.\
SO we need to re  fit the model with out insignificant parameters

```{r}
model_2.1<- lm(y~x1+ x2 -1, data=df1)
summary(model_2.1)
```
```{r}
### Further checking for multicollinearity
vif(model_2.1)
```
The vif is still low, so no multicollineasrity is there.

```{r}
plot(model_2.1)
```
**Comment:** So, the last model is free from multicollinearity with all the significant parameters with a good Adj R^2 value. \
so this model is good enough to work.


## c: PCA 

### PCA
Here we will work with the first data set
```{r}
pca1<- princomp(df[,-1])
summary(pca1)

prcomp(df[,-1])
```
so we will use the `PC1`, component
```{r}
df$z1<- (0.09745891*df$x1+0.99523955*df$x2)
model_pca<- lm(y~z1 ,data=df )
summary(model_pca)
```
```{r}
model_pca_u<- lm(y~z1-1 ,data=df )
summary(model_pca_u)
```


The model becomes
$$ y =0.061016  \times Z_1 = 0.061016  \times (0.09745891*x_1+0.9952395 * 5x_2 )$$

### Sir's algorithm

```{r}
#design matrix
d
# X'X
d_t
# eigen values of X'X
l<- eigen(d_t)$values
# eigen vectors of X'X
L<- eigen(d_t)$vectors 
L
# note: L is a otthogonal matrix

```

```{r}
## Define Z
z=d %*% L
z


## Variability of Z_i
pc_var<- c( var(z[,1]) , var(z[,2]), var(z[,3]) )

## plot of variability explained
barplot((pc_var/sum(pc_var))*100, names.arg = c("z1","z2","z3"),ylim = c(1,100) )
lines((pc_var/sum(pc_var))*100,type = 'b')
```
So the first component explains the most of the variability, so we will take z1 variable for fitting the model.

```{r}
model_p <- lm(df$y~ z[,1])
summary(model_p)
```

The intercept is insignificant, so need to drop that

```{r}
model_pu<- lm(df$y~ 0+ z[,1])
summary(model_pu)
```
So, the model is 
$$ y = -0.061016 \times Z_1$$
```{r}
## Finding estimaton of beta
eta<- -0.061016
beta = L[,1] * eta
round(beta,5)
```
The final model

$$ y= 0.00003 +0.00593 \times X_1  + 0.06073 \times X_2$$

### Ridge Regression

```{r}
## Removing the z1 variable
df<- df[,-4]

library(glmnet)
aa<- 10^seq(2, -3, by = -0.1)
ridge_reg = glmnet(df[,-1], df[,1], nlambda = 25, alpha = 0, family = 'gaussian', lambda = aa)
summary(ridge_reg)
```


```{r}
cv_ridge <- cv.glmnet(as.matrix(df[,-1]), df[,1], alpha = 0, lambda = aa)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
```
so, the optimal choice of c is  1.995262


### Estimation of Beta
```{r}
#X'X
d_t
# I matrix
I = diag(rep(1,3))

#beta
beta= (solve(d_t +  (optimal_lambda*I))) %*% t(d) %*% df[,1]

beta
```

The final model
$$y= 6.129046444 +  0.547649530 \times x_1 +  0.004625073\times x_2 $$

### Ridge Regression Sir's Algorithm
```{r}
# Choice of C


```









# Problem 2

## Importing data
```{r}
df_4<- read.csv("C:\\Users\\souma\\Dropbox\\Mstat_CU\\Sem 2\\Regression_analysis_1\\Data Sets\\problem_set_4_2.csv")
colnames(df_4)[1]<-"pt"
```

```{r}
cor(df_4[,-1])
library(corrplot)
corrplot(cor(df_4[,-1]))
```
```{r}
corrplot(cor(df_4[,c(-1,-2)]))
```
From the plot we observe that the `weight` and `BSA` are highly correlated, so, they may cause multicollinearity.

## Model fitting

```{r}
m_4<- lm(BP ~Age+Weight+BSA+Dur+Pulse+Stress, data = df_4)
summary(m_4)


```
```{r}
summary(influence.measures(m_4))
```
## Vif
```{r}
library(car)
vif(m_4)
```
## CN
```{r}
d_matrix<- model.matrix(m_4)
x<- t(d_matrix) %*% d_matrix

l<- eigen(x)$values

cn<- sqrt(max(l)/min(l))
cn
```
The CN is too large, so there exists multicollieanrity .

## Fit a model without `BSA`
```{r}
m_4.1<- lm(BP ~Age+Weight+Dur+Pulse+Stress, data = df_4)
summary(m_4.1)

```

```{r}
# ------- Lets calculated vif for the model ---------

vif(m_4.1)
```


## Fit a model without `Pluse`
```{r}
m_4.2<- lm(BP ~Age+Weight+Dur+Stress, data = df_4)
summary(m_4.2)
```
**This is a Failure**

## Fit a model without stress
```{r}
m_4.3<- lm(BP ~Age+Weight+Dur, data = df_4)
summary(m_4.3)
```
Again a Bad model, as the VIF for this model has less VIFs so, there is no multicollinearity, so the parameter is ingnificant not for multicollinearity. 


## Fitting a model removing `dur` keeping stress

```{r}
m_4.4<-lm(BP ~Age+Weight+Stress, data = df_4)
summary(m_4.4)
```
## Fitting model without `BSA`  and `Stress`
```{r}
m_4.5<- lm(BP~Age+Weight+Pulse+Dur, data = df_4)
summary(m_4.5)
```
```{r}
#-------VIf-------
vif(m_4.5)
```


## Fitting MOdel without BSA, Stress, DUR
```{r}
m_4.6<- lm(BP ~Age+Weight+Pulse, data = df_4)
summary(m_4.6)
```
```{r}
#------VIF--------
vif(m_4.6)
```


## Again fit the model without `Pluse`
```{r}
m_4.6<- lm(BP ~Age+Weight, data = df_4)
summary(m_4.6)
```

So this is a significant model, lets check  is there still multicollinearity or not
```{r}
#--------vif-----------------
vif(m_4.6)
```
The Vifs are also low

So, the data is good enough to work with further.

**As we would consider this model, the model will be good enough if it statisfies the asuumptions**
```{r}
# ------------------- Residual Analysis -----------------

plot(m_4.6)
```



# Stepwise Regression Model
## FOrward stepwise
```{r}
library(MASS)
int_only<- lm(BP~ 1, data=df_4)
s_m_f<- step(int_only, direction = "forward", scope = formula(m_4))

```
## Coefficients
```{r}
s_m_f$coefficients
```
## Backward Stepwise
```{r}
s_m_b<- step(m_4, direction = "backward", scope = formula(int_only))
```
## Both side
```{r}
s_m<- step(int_only, direction = "both", scope = formula(m_4))
```
```{r}
s_m$coefficients
summary(s_m)
```
```{r}
vif(s_m)
```
```{r}
## CN
a<- model.matrix(s_m)
a<- t(a)%*%a
a<- eigen(a)$values
cn<- sqrt(max(a)/min(a))
cn
```
Multicollinearity is still there.